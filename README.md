# ComputerVision-deep-classification-cnn-transfer


---

## ğŸ§  Models Implemented

### ğŸ”¹ SimpleCNN
- 5 convolutional blocks
- 2 fully connected layers
- Optional dropout (0.3 and 0.5)

### ğŸ”¹ ResidualCNN
- Based on SimpleCNN
- Additional residual blocks after Conv2 and Conv4

### ğŸ”¹ MobileNetV2 (Transfer Learning)
- Scenario 1: Only final FC layer trained
- Scenario 2: Last two convolutional blocks + FC layer fine-tuned

---

## ğŸ”§ Training Settings

- Optimizer: Adam
- Loss: CrossEntropyLoss
- Epochs: 50
- Learning rates: 0.0001 / 0.001 / 0.005
- Batch sizes: 32 / 64

---

## ğŸ“Š Results Summary

| Model              | Val Accuracy | Test Accuracy |
|-------------------|--------------|----------------|
| SimpleCNN          | 58.91%       | 50.91%         |
| ResidualCNN        | 63.27%       | 50.18%         |
| MobileNetV2 (S1)   | 88.00%       | 74.91%         |
| MobileNetV2 (S2)   | **90.18%**   | **79.27%**     |

---

## ğŸ“ˆ Key Insights

- Transfer learning with MobileNetV2 dramatically outperformed scratch CNNs.
- Fine-tuning deeper layers (Scenario 2) improved test accuracy.
- Dropout rate of 0.3 helped generalization in both custom CNNs.
- Residual connections improved validation accuracy, but not test generalization.

---

## ğŸ‘¨â€ğŸ’» Author

**Ufuk Cefaker**  
Computer Vision Lab - Hacettepe University  
Student ID: 2220356171

---

## ğŸ“„ Report

You can find the full detailed report [here](./report.pdf).

